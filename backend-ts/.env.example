# ===============================================================================
# AFM Precificação - TypeScript Backend Configuration
# ===============================================================================
#
# This file contains all configuration options for the TypeScript search backend.
# Copy this file to .env and adjust values for your environment.
#

# ===============================================================================
# CORE SERVER CONFIGURATION
# ===============================================================================

# Port for the TypeScript backend server
PORT=3001

# Base URL of the Python backend (used for fallback and Python-only mode)
PYTHON_API_BASE_URL=http://localhost:8000

# Node environment: development | production | test
NODE_ENV=development

# Log level: debug | info | warn | error
LOG_LEVEL=info

# Enable request logging (true | false)
ENABLE_REQUEST_LOGGING=true

# Request timeout in milliseconds
REQUEST_TIMEOUT_MS=30000

# ===============================================================================
# RUNTIME PROFILE
# ===============================================================================
#
# Selects resource management profile:
# - 'default': Normal development/production (higher limits, flexible timeouts)
# - 'free_tier': Optimized for low-resource cloud environments
#   (512MB-1GB RAM, shared CPU, conservative timeouts, smaller limits)
#
# When 'free_tier' is selected:
#   - MAX_TOP_K defaults to 30 (vs 50)
#   - MAX_BATCH_SIZE defaults to 20 (vs 50)
#   - TS_FALLBACK_TIMEOUT_MS should be 3000+ (conservative)
#
RUNTIME_PROFILE=default

# ===============================================================================
# SEARCH ENGINE MODE
# ===============================================================================
#
# Controls which search engine is used and how:
#
# - 'ts' (RECOMMENDED FOR PRODUCTION):
#   TypeScript engine is PRIMARY with automatic Python fallback.
#   If TS fails or times out, automatically falls back to Python (transparent).
#   Best for: production with safety net, gradual migration from Python.
#
# - 'python' (LEGACY/EMERGENCY):
#   Pure Python backend only. No TypeScript engine involved.
#   Best for: emergency rollback, testing Python-only behavior.
#
# - 'dual' (VALIDATION/QA):
#   Python is PRIMARY (authoritative response to client).
#   TypeScript runs in SHADOW mode (sample % of requests).
#   Logs comparison metrics (Jaccard similarity, rank diff, score MAE).
#   Best for: validating TS engine quality before cutover, A/B testing.
#
SEARCH_ENGINE_MODE=ts

# ===============================================================================
# DATASET CONFIGURATION
# ===============================================================================

# Path to the dataset JSON file for the TypeScript search engine
# This file should be generated by: python backend/scripts/dump_dataset_for_ts.py
# Relative paths are resolved from the repository root.
DATASET_PATH=data/dataset_ts.json

# ===============================================================================
# TYPESCRIPT ENGINE FALLBACK (applies when SEARCH_ENGINE_MODE=ts)
# ===============================================================================

# Enable automatic Python fallback when TS engine fails (1 | 0)
# When enabled: TS errors/timeouts trigger seamless fallback to Python
# When disabled: TS errors are propagated to client (HTTP 500)
TS_FALLBACK_ENABLED=1

# Timeout for TS engine in milliseconds before triggering Python fallback
# Recommended values:
#   - Development: 2500ms (fast feedback)
#   - Free tier: 3000ms (conservative, shared CPU)
#   - Production: 2500ms (balanced)
TS_FALLBACK_TIMEOUT_MS=2500

# ===============================================================================
# DUAL MODE CONFIGURATION (applies when SEARCH_ENGINE_MODE=dual)
# ===============================================================================

# Sample rate for dual-mode comparisons (0-100 percent)
# Controls what % of requests execute both Python and TS for comparison.
#
# Examples:
#   - 0: Never compare (effectively same as 'python' mode)
#   - 10: Compare 10% of requests (recommended for production validation)
#   - 100: Compare all requests (high overhead, for testing only)
#
DUAL_SAMPLE_RATE=10

# ===============================================================================
# EMBEDDING PROVIDER CONFIGURATION
# ===============================================================================
#
# Controls which provider is used for semantic embeddings:
#
# - 'mock': Deterministic stub provider (no external API calls)
#   Best for: development, testing, free-tier without semantic search
#   Cost: FREE, Latency: <1ms
#
# - 'openai': OpenAI Embeddings API (text-embedding-3-small)
#   Best for: production with budget, high-quality embeddings
#   Cost: ~$0.02/1M tokens, Latency: ~100-300ms
#   Requires: OPENAI_API_KEY
#
# - 'hf': Hugging Face Inference API (sentence-transformers/all-MiniLM-L6-v2)
#   Best for: budget-friendly semantic search, free-tier compatible
#   Cost: FREE (with rate limits) or paid tier
#   Requires: HF_API_KEY
#
# - 'none': Disable embeddings (semantic search will throw error)
#   Best for: pure TF-IDF mode only
#
EMBEDDINGS_PROVIDER_MODE=mock

# ===============================================================================
# CROSS-ENCODER PROVIDER CONFIGURATION
# ===============================================================================
#
# Controls which provider is used for semantic reranking:
#
# - 'mock': Deterministic stub provider (no external API calls)
#   Best for: development, testing, free-tier without reranking
#   Cost: FREE, Latency: <1ms
#
# - 'hf': Hugging Face Inference API (cross-encoder/ms-marco-MiniLM-L-6-v2)
#   Best for: production reranking, free-tier compatible
#   Cost: FREE (with rate limits) or paid tier
#   Requires: HF_API_KEY
#
# - 'none': Disable cross-encoder (uses constant scores for reranking)
#   Best for: TF-IDF + embeddings only, no reranking
#
CROSS_ENCODER_PROVIDER_MODE=mock

# ===============================================================================
# HUGGING FACE CONFIGURATION
# ===============================================================================

# Hugging Face API key (required when using 'hf' providers)
# Get your key at: https://huggingface.co/settings/tokens
HF_API_KEY=

# Hugging Face Inference API base URL
HF_API_URL=https://api-inference.huggingface.co

# Embedding model for Hugging Face provider
# Recommended: sentence-transformers/all-MiniLM-L6-v2 (384 dimensions, fast)
# Alternatives:
#   - sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 (multilingual)
#   - sentence-transformers/all-mpnet-base-v2 (768 dimensions, slower but better)
HF_EMBEDDINGS_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Cross-encoder model for Hugging Face provider
# Recommended: cross-encoder/ms-marco-MiniLM-L-6-v2 (fast, good quality)
# Alternatives:
#   - cross-encoder/ms-marco-MiniLM-L-12-v2 (slower but better)
HF_CROSS_ENCODER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2

# ===============================================================================
# OPENAI CONFIGURATION
# ===============================================================================

# OpenAI API key (required when EMBEDDINGS_PROVIDER_MODE=openai)
# Get your key at: https://platform.openai.com/api-keys
OPENAI_API_KEY=

# OpenAI API base URL (change only if using a proxy)
OPENAI_BASE_URL=https://api.openai.com/v1

# OpenAI embedding model
# Recommended: text-embedding-3-small (1536 dimensions, $0.02/1M tokens)
# Alternatives:
#   - text-embedding-3-large (3072 dimensions, $0.13/1M tokens, higher quality)
#   - text-embedding-ada-002 (1536 dimensions, legacy)
OPENAI_EMBEDDINGS_MODEL=text-embedding-3-small

# ===============================================================================
# EMBEDDING CLIENT CONFIGURATION (Generic Factory)
# ===============================================================================
#
# Controls which embedding client implementation is used by the factory.
# This is separate from EMBEDDINGS_PROVIDER_MODE (which controls the search engine).
#
# Supported providers:
# - 'local': LocalEmbeddingClient (DEFAULT)
#   * Runs embeddings locally using @huggingface/transformers (Transformers.js)
#   * Model: Xenova/paraphrase-multilingual-MiniLM-L12-v2 (384 dimensions)
#   * No HTTP calls, no Python, runs on CPU
#   * First call downloads model (~60MB, cached to ~/.cache/huggingface/)
#   * Performance: ~10-20ms per embedding (after model load)
#   * Cost: FREE (CPU only)
#   * Privacy: 100% local (no data sent to external APIs)
#   * Best for: development, testing, low-volume production, privacy-critical apps
#
# - 'openai': OpenAI Embeddings API (HTTP)
#   * Requires: EMBEDDING_API_KEY, EMBEDDING_API_URL
#   * Model: text-embedding-3-small (1536 dimensions)
#   * Performance: ~100-300ms per embedding (network latency)
#   * Cost: $0.02/1M tokens (~$0.000002 per embedding)
#   * Best for: high-volume production, enterprise apps
#
# - 'azure': Azure OpenAI Embeddings API (HTTP)
#   * Requires: EMBEDDING_API_KEY, EMBEDDING_API_URL, AZURE_DEPLOYMENT_NAME
#   * Model: deployment-specific (usually text-embedding-ada-002)
#   * Performance: ~100-300ms per embedding
#   * Cost: varies by Azure pricing
#   * Best for: enterprise with Azure infrastructure
#
# - 'custom': Custom HTTP endpoint (OpenAI-compatible)
#   * Requires: EMBEDDING_API_KEY, EMBEDDING_API_URL, EMBEDDING_MODEL_NAME
#   * Performance: depends on your service
#   * Best for: self-hosted embedding services
#
EMBEDDING_PROVIDER=local

# Embedding API key (ONLY used when EMBEDDING_PROVIDER is 'openai', 'azure', or 'custom')
# Not needed for 'local' provider
EMBEDDING_API_KEY=

# Embedding API base URL (ONLY used for HTTP providers)
# - OpenAI: https://api.openai.com/v1
# - Azure: https://{resource}.openai.azure.com
# - Custom: https://your-service.com/api
# Not needed for 'local' provider
EMBEDDING_API_URL=https://api.openai.com/v1

# Embedding model name (used by HTTP providers)
# - OpenAI: text-embedding-3-small, text-embedding-3-large
# - Azure: deployment name (e.g., 'text-embedding-ada-002')
# - Custom: model identifier
# - Local: hardcoded to Xenova/paraphrase-multilingual-MiniLM-L12-v2 (384 dims)
EMBEDDING_MODEL_NAME=text-embedding-3-small

# Expected embedding dimension (for validation)
# Common values:
#   - 384: Local (Xenova/paraphrase-multilingual-MiniLM-L12-v2), sentence-transformers/all-MiniLM-L6-v2
#   - 768: BERT-based models
#   - 1536: OpenAI text-embedding-3-small, text-embedding-ada-002
#   - 3072: OpenAI text-embedding-3-large
# Note: Local provider always uses 384 dimensions
EMBEDDING_DIMENSION=384

# Azure OpenAI-specific settings (only needed if EMBEDDING_PROVIDER=azure)
# Azure API version (e.g., 2023-05-15, 2024-02-01)
AZURE_API_VERSION=2023-05-15

# Azure deployment name (created in Azure portal)
# If empty, will use EMBEDDING_MODEL_NAME
AZURE_DEPLOYMENT_NAME=

# ===============================================================================
# REQUEST LIMITS (RESOURCE MANAGEMENT)
# ===============================================================================

# Maximum top_k value allowed in search requests
# Requests with larger top_k will be clamped to this value
# Default: 50 (default profile), 30 (free_tier profile)
# Set to override profile defaults
MAX_TOP_K=

# Maximum number of queries in batch requests (/buscar-lote-inteligente)
# Requests exceeding this limit will be rejected with HTTP 400
# Default: 50 (default profile), 20 (free_tier profile)
# Set to override profile defaults
MAX_BATCH_SIZE=

# Timeout for external provider API calls (embeddings, cross-encoder) in milliseconds
# Prevents hanging on slow/unavailable external APIs
PROVIDER_TIMEOUT_MS=2000

# ===============================================================================
# OBSERVABILITY & DEBUG
# ===============================================================================

# Enable debug info in search results (true | false)
# When enabled, response includes:
#   - Timing breakdowns for each pipeline stage
#   - Query domain classification
#   - Domain distribution in results
#   - Engine metadata (which engine served, fallback info)
#
# Adds ~5-10% overhead. Recommended for development and troubleshooting.
ENABLE_DEBUG_INFO=false

# ===============================================================================
# EXAMPLE CONFIGURATIONS
# ===============================================================================
#
# 1. LOCAL DEVELOPMENT (cheap, fast iteration):
#    RUNTIME_PROFILE=default
#    SEARCH_ENGINE_MODE=ts
#    EMBEDDINGS_PROVIDER_MODE=mock
#    CROSS_ENCODER_PROVIDER_MODE=mock
#    EMBEDDING_PROVIDER=local
#    ENABLE_DEBUG_INFO=true
#
# 2. FREE TIER PRODUCTION (no external APIs):
#    RUNTIME_PROFILE=free_tier
#    SEARCH_ENGINE_MODE=ts
#    TS_FALLBACK_TIMEOUT_MS=3000
#    EMBEDDINGS_PROVIDER_MODE=mock
#    CROSS_ENCODER_PROVIDER_MODE=mock
#    ENABLE_DEBUG_INFO=false
#
# 3. FREE TIER WITH SEMANTIC SEARCH (HuggingFace):
#    RUNTIME_PROFILE=free_tier
#    SEARCH_ENGINE_MODE=ts
#    TS_FALLBACK_TIMEOUT_MS=3000
#    EMBEDDINGS_PROVIDER_MODE=hf
#    CROSS_ENCODER_PROVIDER_MODE=hf
#    HF_API_KEY=your_token_here
#    ENABLE_DEBUG_INFO=false
#
# 4. PRODUCTION WITH OPENAI:
#    RUNTIME_PROFILE=default
#    SEARCH_ENGINE_MODE=ts
#    TS_FALLBACK_TIMEOUT_MS=2500
#    EMBEDDINGS_PROVIDER_MODE=openai
#    CROSS_ENCODER_PROVIDER_MODE=hf
#    OPENAI_API_KEY=your_key_here
#    HF_API_KEY=your_token_here
#    ENABLE_DEBUG_INFO=false
#
# 5. VALIDATION/QA (dual mode):
#    RUNTIME_PROFILE=default
#    SEARCH_ENGINE_MODE=dual
#    DUAL_SAMPLE_RATE=10
#    EMBEDDINGS_PROVIDER_MODE=hf
#    CROSS_ENCODER_PROVIDER_MODE=hf
#    HF_API_KEY=your_token_here
#    ENABLE_DEBUG_INFO=true

