# Local Embedding Client - DocumentaÃ§Ã£o

## VisÃ£o Geral

O **LocalEmbeddingClient** Ã© uma implementaÃ§Ã£o de cliente de embeddings que roda **100% localmente** em Node.js, sem dependÃªncias de HTTP ou Python. Usa a biblioteca [@huggingface/transformers](https://huggingface.co/docs/transformers.js) (Transformers.js) para executar modelos de linguagem diretamente no backend TypeScript.

## CaracterÃ­sticas

- âœ… **Zero HTTP**: NÃ£o faz chamadas de rede, tudo roda localmente
- âœ… **Zero Python**: NÃ£o depende de servidor Python ou subprocess
- âœ… **MultilÃ­ngue**: Modelo otimizado para PortuguÃªs, InglÃªs e 50+ idiomas
- âœ… **Cache automÃ¡tico**: Modelo baixado uma vez e armazenado em `~/.cache/huggingface/`
- âœ… **Singleton pattern**: Carregamento lazy do modelo (apenas na primeira chamada)
- âœ… **Batch processing**: Embeddings em lote para melhor performance
- âœ… **Type-safe**: Interface TypeScript completa

## InstalaÃ§Ã£o

```bash
npm install @huggingface/transformers
```

## Uso BÃ¡sico

```typescript
import { LocalEmbeddingClient } from './infra/localEmbeddingClient';

// Criar cliente (nÃ£o carrega modelo ainda)
const client = new LocalEmbeddingClient();

// Gerar embedding para texto
const embedding = await client.embed("Lavadora de piso industrial");
console.log(embedding.length); // 384

// Similaridade entre textos
const emb1 = await client.embed("Lavadora de piso");
const emb2 = await client.embed("MÃ¡quina de lavar chÃ£o");
const similarity = LocalEmbeddingClient.cosineSimilarity(emb1, emb2);
console.log(similarity); // 0.85 (muito similares)
```

## Uso AvanÃ§ado

### Batch Processing

```typescript
const texts = [
  "Enceradeira industrial",
  "Lavadora de alta pressÃ£o",
  "Aspirador de Ã¡gua e pÃ³"
];

// Mais eficiente que mÃºltiplas chamadas embed()
const embeddings = await client.embedBatch(texts);
console.log(embeddings.length); // 3
```

### Modelo Customizado

```typescript
// Usar outro modelo do HuggingFace
const client = new LocalEmbeddingClient('Xenova/all-MiniLM-L6-v2');
console.log(client.modelName); // Xenova/all-MiniLM-L6-v2
console.log(client.dimension); // 384
```

### Factory Pattern

```typescript
import { createEmbeddingClient } from './infra/localEmbeddingClient';

const client = createEmbeddingClient('local', {
  modelName: 'Xenova/paraphrase-multilingual-MiniLM-L12-v2'
});
```

## Modelo PadrÃ£o

**Xenova/paraphrase-multilingual-MiniLM-L12-v2**

- **DimensÃµes**: 384
- **Idiomas**: 50+ (incluindo PortuguÃªs e InglÃªs)
- **Tamanho**: ~60MB (download Ãºnico, depois cached)
- **Performance**: ~10-20ms por embedding (CPU)
- **PropÃ³sito**: Semantic similarity, information retrieval, clustering

Modelo convertido para ONNX pela biblioteca Transformers.js, compatÃ­vel com Node.js/browser.

ğŸ“š **ReferÃªncias**:
- [HuggingFace Model Card](https://huggingface.co/Xenova/paraphrase-multilingual-MiniLM-L12-v2)
- [Original Model (sentence-transformers)](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2)

## Performance

### Benchmark Local (CPU Intel Core i7)

| OperaÃ§Ã£o | Tempo | ObservaÃ§Ãµes |
|----------|-------|-------------|
| Primeiro embedding | ~90s | Inclui download + carregamento do modelo |
| Embeddings subsequentes | ~10-20ms | Modelo em memÃ³ria (cache) |
| Batch (3 textos) | ~15ms | 5ms por item (3x mais rÃ¡pido que sequencial) |

### ComparaÃ§Ã£o: Local vs HTTP

| MÃ©trica | LocalEmbeddingClient | HttpEmbeddingClient |
|---------|---------------------|---------------------|
| **LatÃªncia** | 10-20ms | 100-300ms (rede + API) |
| **Custo** | Zero (CPU local) | Pago (OpenAI: $0.13/1M tokens) |
| **Privacidade** | 100% local | Dados enviados para API |
| **DependÃªncias** | @huggingface/transformers | undici + API key |
| **Escalabilidade** | Limitado por CPU | Limitado por rate limits |
| **Setup** | `npm install` | API key + configuraÃ§Ã£o |

**RecomendaÃ§Ã£o**:
- Use **Local** para: desenvolvimento, testes, baixo volume, privacidade crÃ­tica
- Use **HTTP** para: produÃ§Ã£o de alto volume, modelos grandes (GPT-4), baixa latÃªncia crÃ­tica

## Arquitetura

### Pipeline de ExecuÃ§Ã£o

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LocalEmbeddingClient.embed("Lavadora de piso")         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ getExtractor() - Lazy loading com singleton             â”‚
â”‚ - Primeira chamada: carrega modelo de ~/.cache/         â”‚
â”‚ - Chamadas seguintes: retorna instÃ¢ncia cached          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Transformers.js pipeline('feature-extraction')          â”‚
â”‚ - TokenizaÃ§Ã£o (BERT tokenizer)                          â”‚
â”‚ - Inference (ONNX runtime)                              â”‚
â”‚ - Mean pooling (mÃ©dia sobre tokens)                     â”‚
â”‚ - L2 normalization (cosine = dot product)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Return: Float32Array[384] â†’ number[]                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Cache de Modelos

Modelos sÃ£o baixados automaticamente no primeiro uso e salvos em:

**Linux/Mac**: `~/.cache/huggingface/transformers/`
**Windows**: `C:\Users\<user>\.cache\huggingface\transformers\`

Para limpar cache:
```bash
# Linux/Mac
rm -rf ~/.cache/huggingface/transformers/

# Windows
Remove-Item -Recurse -Force "$env:USERPROFILE\.cache\huggingface\transformers"
```

## Interface EmbeddingClient

O `LocalEmbeddingClient` implementa a interface `EmbeddingClient`, permitindo intercambialidade com `HttpEmbeddingClient`:

```typescript
interface EmbeddingClient {
  embed(text: string): Promise<number[]>;
  readonly dimension: number;
  readonly modelName: string;
}
```

Isso permite trocar implementaÃ§Ãµes sem alterar cÃ³digo:

```typescript
// Desenvolvimento: local
const client: EmbeddingClient = new LocalEmbeddingClient();

// ProduÃ§Ã£o: HTTP
const client: EmbeddingClient = new OpenAIEmbeddingClient({
  apiKey: process.env.OPENAI_API_KEY
});

// CÃ³digo usa a mesma interface
const embedding = await client.embed(text);
```

## Testes

### Executar Testes

```bash
npm run test:embeddings:local
```

### Testes IncluÃ­dos

1. âœ… **Single embedding**: Gerar embedding para um texto
2. âœ… **Cached model**: Verificar que segunda chamada Ã© rÃ¡pida (modelo em memÃ³ria)
3. âœ… **Semantic similarity**: Calcular similaridade entre dois textos
4. âœ… **Similarity ranking**: Confirmar que textos similares tÃªm score maior
5. âœ… **Batch embedding**: Gerar mÃºltiplos embeddings de uma vez
6. âœ… **Error handling**: Rejeitar textos vazios com erro claro

### Output Esperado

```
ğŸ§ª Testing LocalEmbeddingClient with Transformers.js

Model: Xenova/paraphrase-multilingual-MiniLM-L12-v2
Dimension: 384

Test 1: Single text embedding
==============================
Input: "Lavadora de piso industrial automÃ¡tica"
Output: [-0.0230, -0.0125, 0.0426, -0.0291, 0.0023...]
Dimension: 384
Time: 97569ms (includes model loading)

Test 2: Cached model (second call)
===================================
Input: "Aspirador de pÃ³ profissional"
Output: [-0.0419, 0.0336, -0.0323, 0.0007, -0.0594...]
Time: 13ms (model cached)

âœ… All tests passed!
```

## Troubleshooting

### Erro: "Cannot find module '@huggingface/transformers'"

```bash
npm install @huggingface/transformers
```

### Erro: "Failed to load embedding model"

1. Verifique conexÃ£o com internet (download do modelo)
2. Verifique espaÃ§o em disco (modelo ~60MB)
3. Limpe cache e tente novamente:
   ```bash
   rm -rf ~/.cache/huggingface/transformers/
   ```

### Performance Lenta (>1s por embedding)

1. **CPU overhead**: Primeira chamada baixa modelo (~90s normal)
2. **Modelo nÃ£o cached**: Verifique se `~/.cache/huggingface/` tem o modelo
3. **Batch processing**: Use `embedBatch()` para mÃºltiplos textos

### Erro: "Unexpected embedding dimension"

Modelo retornou dimensÃ£o diferente de 384. PossÃ­veis causas:
- Modelo customizado com dimensÃ£o diferente
- CorrupÃ§Ã£o no cache (delete `~/.cache/huggingface/`)

## Roadmap

- [ ] Suporte a GPU via ONNX runtime
- [ ] QuantizaÃ§Ã£o INT8 para reduzir tamanho do modelo
- [ ] Cache de embeddings em disco (evitar recomputaÃ§Ã£o)
- [ ] Modelos multilÃ­ngues menores (<30MB)
- [ ] Batch size dinÃ¢mico (auto-tuning)

## ComparaÃ§Ã£o com Python

### Python (sentence-transformers)

```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
embedding = model.encode("Lavadora de piso industrial")
print(embedding.shape)  # (384,)
```

### TypeScript (Transformers.js)

```typescript
import { LocalEmbeddingClient } from './infra/localEmbeddingClient';

const client = new LocalEmbeddingClient();
const embedding = await client.embed("Lavadora de piso industrial");
console.log(embedding.length); // 384
```

**DiferenÃ§as**:
- **Python**: Usa PyTorch/TensorFlow (mais rÃ¡pido em GPU)
- **TypeScript**: Usa ONNX Runtime (otimizado para CPU, funciona em browser)
- **Embeddings**: IdÃªnticos (mesmo modelo ONNX convertido)

## ReferÃªncias

- [Transformers.js Documentation](https://huggingface.co/docs/transformers.js)
- [ONNX Runtime](https://onnxruntime.ai/)
- [Sentence Transformers](https://www.sbert.net/)
- [HuggingFace Model Hub](https://huggingface.co/models)

---

**Criado por**: GitHub Copilot  
**Ãšltima atualizaÃ§Ã£o**: 2024-01-XX  
**VersÃ£o**: 1.0.0
